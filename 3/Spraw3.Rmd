---
title: "Sprawozdanie 3 - ASzCz"
author: "Kacper Budnik"
date: "2023-05-27"
output:
  pdf_document: 
    toc: true
    number_sections: true
    extra_dependencies: ["polski", "mathtools", "amsthm", "amssymb", "icomma", "upgreek", "xfrac", "float", "hyperref", "caption", "enumitem"]
fontsize: 12pt
---



```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, eval = TRUE, fig.pos = "H", dev.args=list(encoding="CP1257.enc"), fig.path='figure/', fig.align='center', fig.pos='H',fig.width=5, fig.height=3)
```

```{r, echo=FALSE}
library(EnvStats)
library(ggplot2)
library(dplyr)
library(tidyr)
library(vcd)
library(knitr)
library(TSAFBook)
library(astsa)
library(forecast)
library(gridExtra)
library(DescTools)
library(tseries)
library(kableExtra)
library(itsmr)
library(ggfortify)
```

```{r, echo=FALSE}
month.name=c("Styczeń", "Luty", "Marzec", "Kwiecień", "Maj", "Czerwiec", "Lipiec", "Sierpień", "Wrzesień", "Październik", "Listopad", "Grudzień")
```
\newpage
\section{Dopasowanie modeli autoregresji do szeregu globtemp.}
\subsection{Cel i dane.}
Rozpatrywane dane -- \verb|globtemp| -- pochodzących z pakietu \verb|astsa|. Przedstawiają one odchylenia średniej rocznej temperatury od temperatury średniej globalnej w latach 1951-1980. Pierwszy pomiar pochodzi z<!-- `r month.name[start(globtemp)[2]]`--> roku `r start(globtemp)[1]`, natomiast ostatni z  `r end(globtemp)[1]`. Celem pierwszej części sprawozdania będzie przeprowadzenie kompletnej analizy związaniem z dopasowaniem i diagnostyką modeli autoregresji (AR).
\subsection{Weryfikowanie stacjonarności danych.}
Przed przystąpieniem do analizy, sprawdźmy, czy rozpatrywany szereg jest stacjonarny. Zacznijmy od przyjrzenia się danym na wykresie.
```{r, fig.cap="Wykresy danych podstawowych oraz ich funkcje autokorelacji i częściowej autokorelacji.",fig.width=6, fig.height=7}
#X=globtemp
#par(mfrow=c(2,1), mar=c(4,4,1,1))
#plot(X)
#plot(acf(X, plot=F), main = "")
ggtsdisplay(globtemp)
```
Jak możemy zauważyć, widoczny jest silny trend, w dodatku funkcja autokorelacji zanika powoli, zatem rozpatrywany szereg nie jest szeregiem stacjonarnym. Dlatego w dalszej części zadania rozpatrywaliśmy zróżnicowane dane z krokiem 1, gdyż nie zauważyliśmy żadnych zachowań sezonowych. Dla tych danych wykresy analogiczne jak wcześniej prezentują się następująco.
```{r, fig.cap="Wykresy danych po różnicowaniu oraz ich funkcje autokorelacji i częściowej autokorelacji.",fig.width=6, fig.height=7}
X=diff(globtemp, differences = 1)
#plt1<-autoplot(X) + ylab("")
#plt2<-ggAcf(X) + ggtitle("")
#grid.arrange(plt1,plt2,ncol=2)
ggtsdisplay(X)
tmp<-sapply(1:13, function(h) adf.test(X, k=h)$p.val)
```
Po zróżnicowaniu nie zauważyliśmy trendu oraz funkcje autokorelacji jak i częściowej autokorelacji szybko zanikają od 0. Dlatego możemy podejrzewać, że mamy już do czynienia z szeregiem stacjonarnym. Dodatkowo test ADF (Augmented Dickey-Fuller test) zwrócił p-value na poziomie `r adf.test(X)$p.val`<0.05 (dla opóźnienia `r adf.test(X)$parameter`, dla innych podobnie), zatem mamy podstawy do odrzucenia hipotezy o niestacjonarności zróżnicowanych danych. Dodatkowo nie widać żadnych niejednolitości w wariancji, jednak możemy spróbować sprawdzić to przy użyciu formalnych testów. W tym celu skorzystaliśmy z testowania wstecznego. Podzieliliśmy dane na części i sprawdziliśmy czy wariancje w każdej części są takie same. W tym celu wykorzystaliśmy funkcję \verb|var.test|. W poniższej tabeli znajdują się p-wartości wykonanych testów dla podziału na 5 części.
```{r, fig.cap="p-wartość testów na zgodność wariancji. Numer kolumny i wiersza oznacza które części porównywaliśmy ze sobą."}
k=5
tmp=split(X, 1:k)
int=floor(length(X)/k)
mat=matrix(nrow=k,ncol=k)
mat[lower.tri(mat)]<-sapply(
  tmp,FUN=function(x) sapply(
    tmp, FUN=function(y) var.test(x,y)$p.val
    )
  )[lower.tri(mat)]
mat[upper.tri(mat, diag = TRUE)]<-sapply(
  1:k, FUN=function(x) sapply(
    1:k, FUN=function(y) var.test(X[((x-1)*int+1):(x*int)],X[((y-1)*int+1):(y*int)])$p.val
    )
  )[upper.tri(mat, diag = TRUE)]
row.names(mat)<-1:k
colnames(mat)<-1:k
mat=cbind(1:k,mat)
kable(round(mat, digits = 4)) %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")
```
W komórkach nad diagonalą znajdują się wyniki przy podziale próby na części zawierające kolejne obserwacje. W dolnej części podział nastąpił przy pomocy funkcji \verb|split| (w każdej części należała co `r k` obserwacja). W każdym przypadku wartość była większa od standardowego progu $\alpha=0.05$ zatem nie mamy podstaw do odrzucenia hipotezy o homoskedastyczności. Jednak warto mieć na uwadze], że w czwartym okresie wyniki są znacznie niższe niż w pozostałych.
\subsection{Wybór modelu AR}
Patrząc na drugi wykres, a dokładnie na funkcję PACF dla zróżnicowanych danych, możemy postulować, że odpowiednim modelem dla danych będzie AR(3), model autoregresji rzędu 3. Jednak by nie opierać się jedynie na metodach graficznych skorzystamy z kryteriów AIC oraz FPE. Implementacja jego wygląda następująco.
```{r, echo=T}
FPE<-function(p) arima(X,order=c(p,0,0))$sigma2*(length(X)+p)/(length(X)-p)
```
Porównajmy na wykresach powyższe kryterium z kryterium Akkaike (AIC).
```{r,  fig.cap="Wartości kryteriów informacyjnych dla danego rzędu modelu."}

p.max=10
par(mfrow=c(1,2), mar=c(2,2,1,0))
plot(0:p.max,sapply(0:p.max, FUN=function(p) arima(X,order=c(p,0,0))$aic), xlab = "", ylab="", main="AIC")
plot(0:p.max, sapply(0:p.max, FUN=FPE), xlab = "", ylab="", main="FPE")

```
Możemy zauważyć, że najmniejszą wartość oba kryteria przyjmują dla $p=3$, czyli dla tego samego rzędu modelu, który wybraliśmy interpretując funkcję PACF. Jednak możemy zobaczyć, że wartość kryteriów w tym punkcie jest zbliżona do wartości dla rzędu $p=5$. Jeśli ponownie spojrzelibyśmy na wykres PACF zauważylibyśmy, że dla tego opóźnienia słupek częściowej autokorelacji jest niewiele niższy niż niebieskie linie, zatem przy pomocy każdej z tych trzech metod otrzymaliśmy podobne wyniki. 
\subsection{Wyznaczenie parametrów modelu AR.}
W poprzednim krok wyznaczyliśmy rząd modelu $p=3$. Jednak ponieważ wyniki (kryteria) zwracały niewiele większe wyniki dla rzędu $p=5$ rozpatrywaliśmy oba przypadki. Do estymacji parametrów modelu posłużyliśmy się funkcją \verb|ar| z pakiety \verb|stats|, przy użyciu dwóch metod. Pierwszą metodą była metoda yule-walker'a, natomiast drugą była metoda największej wiarygodności. Wyniki przedstawiliśmy w poniższych tabelach.
```{r}
p=3
tmp=rbind(ar(X, aic = F, order.max = p, method = "yw")$ar,
          ar(X, aic = F, order.max = p, method = "mle")$ar,
          ar(X, aic = F, order.max = p, method = "burg")$ar,
          ar(X, aic = F, order.max = p, method = "ols")$ar)
rownames(tmp)<-c("Yule-Walker", "MLE", "Burg", "OLS")
kable(tmp, caption = paste0("Wyestymowane parametry dla modelu AR(",p,").")) %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")
```
```{r}
p=5
tmp=rbind(ar(X, aic = F, order.max = p, method = "yw")$ar,
          ar(X, aic = F, order.max = p, method = "mle")$ar,
          ar(X, aic = F, order.max = p, method = "burg")$ar,
          ar(X, aic = F, order.max = p, method = "ols")$ar)
rownames(tmp)<-c("Yule-Walker", "MLE", "Burg", "OLS")
kable(tmp, caption = paste0("Wyestymowane parametry dla modelu AR(",p,").")) %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")
```
Możemy zauważyć, że metoda Yule-walker'a dała najmniejsze (co do modułu) estymatory dla pierwszych trzech wartości, niezależnie od wielkości modelu. Jednak estymatory różnią się od siebie co najwyżej na drugim miejscu po przecinku, a w wielu przypadkach różnica jest jeszcze mniejsza. Każdy z estymatorów zwrócił takie same wyniki: dla rzędu $p=3$ wszystkie parametry są statystycznie istotne, natomiast dla rzędu $p=5$ czwarty parametr jest statystycznie nieistotny, gdzie za parametry statystycznie nieistotne uznaliśmy te, dla których przedziały ufności na poziomie $\alpha=0.05$ pokrywają wartość $0$.
```{r, results='hide'}
diag(ar(X, aic = F, order.max = p, method = "yw")$asy.var.coef)
diag(ar(X, aic = F, order.max = p, method = "mle")$asy.var.coef)

a=0.05

# przedziały symetryczne, więc są nieistotne jeśli |\hat\phi|<1.96*sqrt(sigma2*n)

p=3
abs(ar(X, aic = F, order.max = p, method = "yw")$ar)<qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "yw")$asy.var.coef)/sqrt(length(X)))
abs(ar(X, aic = F, order.max = p, method = "mle")$ar)<qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "mle")$asy.var.coef)/sqrt(length(X)))
abs(ar(X, aic = F, order.max = p, method = "burg")$ar)<qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "burg")$asy.var.coef)/sqrt(length(X)))

p=5
abs(ar(X, aic = F, order.max = p, method = "yw")$ar)<qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "yw")$asy.var.coef))/sqrt(length(X))
abs(ar(X, aic = F, order.max = p, method = "mle")$ar)<qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "mle")$asy.var.coef))/sqrt(length(X))
abs(ar(X, aic = F, order.max = p, method = "burg")$ar)<qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "burg")$asy.var.coef))/sqrt(length(X))

```
Przykładowe przedziały ufności dla rzędu $p=5$ oraz metody największej wiarygodności wyglądają następująco.
```{r}
a=0.05
p=5
mat=matrix(nrow=3, ncol=p)
mat[2,]=ar(X, aic = F, order.max = p, method = "mle")$ar
mat[1,]=mat[2,]+qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "mle")$asy.var.coef))/sqrt(length(X))
mat[3,]=mat[2,]-qnorm(1-a/2)*sqrt(diag(ar(X, aic = F, order.max = p, method = "mle")$asy.var.coef))/sqrt(length(X))

rownames(mat)<-c("Górna granica","Wartość estyamtora", "Dolna granica")
colnames(mat)<-sapply(1:p,function(txt) paste0("ar",txt))

kable(tmp, caption = paste0("Przedziały ufności dla parametrów wyestymowanych metodą MLE dla modelu AR(5).")) %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")

```

\subsection{Test białoszumowości dla reszt.}
Możemy jeszcze sprawdzić dopasowanie modelu poprzez testowanie białoszumowości reszt. Podobnie jak w pierwszym sprawozdaniu, wykorzystaliśmy w tym celu test graficzny oraz formalne testy: Box-Pierce i Ljung-Box. P wartośc uzyskaną przy wykorzystaniu testów formalnych zamieszczono w poniższych tabelach.
```{r}
p=3
tmp<-
  sapply(c("Box-Pierce","Ljung-Box"), FUN=function(typ) 
    sapply(c("yule-walker", "burg", "ols", "mle"), FUN=function(met) 
      Box.test(ar(X,aic=F,order.max = p, method = met)$resid, type=typ)$p.val)) %>% t()
colnames(tmp)<-c("Yule-Walker", "MLE", "Burg", "OLS")
kable(tmp, caption = paste0("Testy białoszumowości dla modelu AR(",p,").")) %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")
```
```{r}
p=5
tmp<-
  sapply(c("Box-Pierce","Ljung-Box"), FUN=function(typ) 
    sapply(c("yule-walker", "burg", "ols", "mle"), FUN=function(met) 
      Box.test(ar(X,aic=F,order.max = p, method = met)$resid, type=typ)$p.val)) %>% t()
colnames(tmp)<-c("Yule-Walker", "MLE", "Burg", "OLS")
kable(tmp, caption = paste0("Testy białoszumowości dla modelu AR(",p,").")) %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")
```
W każdym przypadku zwrócono wartość przewyższającą standardowy próg ufności, na poziomie $\alpha=0.05$, zatem nie mamy podstaw do odrzucenia hipotezy o niezależności reszt. Dodatkowo wykonując test graficzny otrzymaliśmy poniższą tabelę.
```{r}
test_vialo<-function(X){
  n=length(X)
  h=floor(n/4)
  af=abs(acf(X,lag=h,plot=FALSE)$acf[-1])
  if(sum(af>1.96/sqrt(n))/h > 0.05) return(FALSE)#(c(0,sum(af<1.96/sqrt(n))/h))
  if(max(af)>2.94/sqrt(n)) return(FALSE)
  return(TRUE)
}


tmp<-
  sapply(c(3,5), FUN=function(p) 
    sapply(c("yule-walker", "burg", "ols", "mle"), FUN=function(met) 
      ar(X,aic=F,order.max = p, method = met)$resid %>%
        na.remove %>% test_vialo
      )) %>% t



colnames(tmp)<-c("Yule-Walker", "MLE", "Burg", "OLS")
rownames(tmp)<-c("AR(3)","AR(5)")
kable(tmp, caption = paste0("Graficzny testy białoszumowości.")) %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")

```
W tabeli wartość \verb|TRUE| oznacza, że próbka przeszła test białoszumowości, więc ponownie nie mamy podstaw do odrzucenia hipotezy o białoszumowości.

Możemy jeszcze sprawdzić, czy reszty mają rozkład normalny. Zacznijmy od testu graficznego. Reszty dla modelu AR(3) wyestymowanego przy pomocy motody Yule-Walker'a porównaliśmy z rozkładem normalnym na wykresie kwantylowym. 
```{r, fig.cap="Reszty modelu AR(3) (dla metody YW)."}
qqnorm(ar(X,aic=F,order.max = 3)$resid)

```
Widzimy, że nasze dane dobrze przybliżają linię prostą. Możemy przyjrzeć się jeszcze testom formalnym.
```{r}
tmp<-
  sapply(c(3,5), FUN=function(p) 
    sapply(c("yule-walker", "burg", "ols", "mle"), FUN=function(met) 
      shapiro.test(na.remove(ar(X,aic=F,order.max = p, method = met)$resid))$p
      )) %>% t

colnames(tmp)<-c("Yule-Walker", "MLE", "Burg", "OLS")
rownames(tmp)<-c("AR(3)","AR(5)")
kable(tmp, caption = "p wartości testu Shapiro Wilka dla różnych modeli.") %>%
  kable_styling( full_width = F, font_size = 12,latex_options = "HOLD_position")
```
Tutaj analogicznie do metody graficznej, nie mamy podstaw do odrzucenia hipotezy o normalności reszt.
\subsection{Predykcja}
Mając już dopasowany model do danych, możemy teraz spróbować przewidzieć zachowanie temperatury w przyszłości. W tym celu skorzystaliśmy z funkcji \verb|predict| z pakietu \verb|stats|. Wyniki prezentują się następująco.

```{r, fig.cap="Dane podstawowe z predykcją na 10 lat"}
Forecast<-predict(ar(X, aic=F, order.max = 3), n.ahead = 10)$pred %>%
  diffinv(xi=globtemp[length(globtemp)])
autoplot(cbind(globtemp,Forecast)) + ylab("Temp")
```
Możemy zauważyć, że według naszej prognozy, w najbliższych latach temperatura delikatnie spadnie, jednak w ciągu 10 lat będzie zachowywać trend dodatni i ostatecznie temepratura będzie wyższa niż w poprzednich okresach. Możemy zobaczyć jak będzie wyglądała predykcja na dłuższy okres.

```{r, fig.cap="Dane podstawowe z predykcją na 50 lat"}
Forecast<-predict(ar(X, aic=F, order.max = 3), n.ahead = 50)$pred %>%
  diffinv(xi=globtemp[length(globtemp)])
autoplot(cbind(globtemp,Forecast)) + ylab("Temp")
```
W tym przypadku prognoza będzie liniowa. Predykcja długoterminowa jest trudniejsza, więc powinna ona przybliżać jedynie trend. Jednak możemy zauważyć, że prognozowany trend jest mniej stromy niż był w ostatnich latach (np. 1980-2010).


\section{Dopasowanie modeli ARMA dla wybranych danych rzeczywistych.}
\subsection{Transformatcja danych i identyfikacjia modelu SARIMA}
<!--W drugim zadaniu postaramy się dopasować model \verb|ARMA| do danych rzeczywistych. Z powodu zalania serwerów Politechniki Wrocławskiej, skutkującym brakiem dostępu do listy możliwych danych, skorzystaliśmy z danych rozpatrywanych w poprzednim sprawozdaniu (z nadzieją, że były one dozwolone). Rozpatrywanymi danymi są dane odnośnie miesięcznej produkcji energii elektrycznej w Polsce. Pochodzą one z pakietu \verb|TSAFBook|. Na wykresie prezentują się następująco-->

W drugim zadaniu postaramy się dopasować model \verb|ARMA| do danych rzeczywistych. Skorzystaliśmy z danych rozpatrywanych w poprzednim sprawozdaniu, zawierających informacje odnośnie miesięcznej produkcji energii elektrycznej w Polsce. Pochodzą one z pakietu \verb|TSAFBook|. Na wykresie prezentują się następująco.
```{r, fig.cap="Przedstawienie analizowanych danych na wykresie."}
autoplot(energia)
```
W analizie porównaliśmy dwa podejście pozwalające na uwzględnienie występującego trendu długoterminowego oraz sezonowości: różnicowanie oraz dekompozycja na bazie modelu parametrycznego (skorzystaliśmy z funkcji \verb|tslm|). Dodatkowo przed różnicowaniem na dane nałożyliśmy transformacje Box'a-Cox'a dla parametru $\lambda=0$ (transformacja logarytmiczna). Wybór tej wartości parametru bazował na wiedzy empirycznej uzyskanej z innych kursów (przykładowo z "Rynków finansowych" i wiedzy o "częstych" własnościach logarytmicznych stóp zwrotu). Dla metody parametrycznej nałożyliśmy tą transformację z parametrem $\lambda=0.75$, gdzie wartość tego parametru wynika z rozważań z poprzedniego sprawozdania (dodatkowo jest to wartość uzyskana metodą największej wiarygodności przy użyciu funkcji \verb|BoxCox.lambda|).

W przypadku różnicowania, wykorzystaliśmy jednokrotne różnicowanie sezonowe z opóźnieniem \verb|lag=12|, dodatkowo jest to rekomendowana liczba różnicowań według funkcji \verb|ndiffs| oraz \verb|nsdiffs|. Natomiast w metodzie parametrycznej rozpatrywaliśmy trend liniowy. Wybór trendu liniowego był oparty na znalezionych danych oraz na tym, że dla większych wartości (2-4) współczynnik przy najwyższej potędze był ujemny (model niefizyczny, pobór energii zacząłby maleć do 0). W wyniku otrzymaliśmy poniższe szeregi.
```{r, results='hide'}
#BoxCox.lambda(energia, method = "loglik")  
# tmp2=tslm(energia ~ poly(trend,2,raw=TRUE) + season)
# tmp1=tslm(energia ~ poly(trend,1,raw=TRUE) + season)
# 
# autoplot(tslm(energia ~ poly(trend,2,raw=TRUE) + season)$residuals)
# autoplot(tslm(energia ~ poly(trend,1,raw=TRUE) + season)$residuals)
#
# ari2<-auto.arima(tmp2$residuals)
# ari1<-auto.arima(tmp1$residuals)
#
# tmp=c(1:10)
# for(l in 1:10){
#   tmp[l]=tslm(energia ~ poly(trend,degree=l,raw=TRUE) + season, lambda = 0.75)$coefficients[l+1]
# }
# 
# tmp

# 
# tmp<-BoxCox(energia, lambda = 0)
# X<-diff(tmp, lag=12)
# shapiro.test(X)$p
# 
# tmp=matrix(c(1:40), nrow=4)
# lamb=c(0,1,0.75,2.5)
# for(l in 1:10){
#   for(m in 1:4){
#     tmp[m,l]=tslm(energia ~ poly(trend,degree=l,raw=TRUE) + season, lambda=lamb[m])$coefficients[l+1]
#   }
# }
# tmp

# 
# tmp=matrix(c(1:40), nrow=4)
# lamb=c(0,1,0.75,2.5)
# for(l in 1:10){
#   for(m in 1:4){
#     tmp[m,l]=min(
#       sapply(
#         1:5,function(o) 
#           adf.test(
#             tslm(energia ~ poly(trend,degree=l,raw=TRUE) + season, lambda=lamb[m])$resid,
#             k=o)$p.val))
#   }
# }
# tmp
# 
# 
# 
# 
#autoplot(BoxCox(energia, lambda = 0.75))




```

```{r, fig.cap="Wykres residuów dla różnicowania sezonowego.", fig.height=7}
X=diff(BoxCox(energia,lambda = 0), lag=12)
ggtsdisplay(X)
```
```{r, fig.cap="Wykres residuów dla dekompozycji parametrycznej.", fig.height=7}
Y=tslm(energia ~ poly(trend,1,raw=TRUE) + season, lambda=0.75)
ggtsdisplay(Y$residuals)
```
Możemy zauważyć, że dla obu metod funkcje autokorelacji, jak i częściowej autokorelacji szybko zanikają do 0. Możemy teraz spróbować zidentyfikować rzędy modeli $AR(p,P)_s$ oraz $MA(q,Q)_s$, gdzie duże litery oznaczają rząd modelu sezonowego, a $s$ -- sezon. W przypadku modelu opartego na różnicowaniu zidentyfikowaliśmy modele: $MA(3,0)_{12}$ oraz $AR(2,1)_{12}$. W przypadku modelu MA można jeszcze rozważyć modele $MA(4,0)_{12}$, $MA(3,1)_{12}$ oraz $MA(4,1)_{12}$, natomiast w przypadku modeli AR mogliśmy jeszcze rozważyć $AR(2,0)_{12}$ lub $AR(14,0)$. Jednak dla modeli MA, opóźnienie funkcji częściowej autokorelacji wynoszące $h=14$ "zignorowaliśmy". Nie daliśmy rady zinterpretować, dlaczego to opóźnienie powinno być istotne (jako dane odnośnie zużycia energii) oraz nie wystaje ono "bardzo" (mniej niż $3\sigma$) poza przedziały istotności.
```{r, results='hide'}
#auto.arima(X)
#tmp=pacf(X)

#0.3272435

#1.96/sqrt(length(X))*3/2

```

W przypadku metody parametrycznej sytuacja jest bardziej jednoznaczna. Otrzymane modele to $MA(3,0)_{12}$ oraz $AR(2,0)_{12}$. Istnieje parę wartości opóźnienia dla obu funkcji, które są na granicy istotności, jednak zważają na wartość opóźnienia dla jakich przyjmują te wartości, nie znaleźliśmy argumentów, które mogły by tłumaczyć tą zależność.



```{r}
# 
# order=c()
# max_order=c(11,11,3,3,3)
# max_order=c(6,6,2,2,3)
# mat=array(dim=max_order)
# for(p in 0:(max_order[1]-1)){
#   for(q in 0:(max_order[2]-1)){
#     for(P in 0:(max_order[3]-1)){
#       for(Q in 0:(max_order[4]-1)){
#         tmp<-Arima(X,order=c(p,0,q), seasonal = c(P,0,Q))
#         mat[p+1,q+1,P+1,Q+1,]<-c(tmp$aic,tmp$bic,tmp$aicc)
#       }
#     }
#   }
# }
# 
# auto.arima()
# 
# 
# 
# 
# tmp=auto.arima(Y$residuals)
# 
# 
# qnorm(0.95)*diag(ar(X, aic = F, order.max = p, method = "mle")$asy.var.coef)/sqrt(length(X))
# 
# 
# Y=tslm(energia ~ poly(trend,1,raw=TRUE) + season, lambda=0.75)
# tmp=auto.arima(Y$residuals, ic="aic")
# 
# tmp$arma  
# tmp=arima(Y,order=c(2,0,1), seasonal=c(1,0,1))
# a=0.1
# auto.arima(Y$residuals, ic="aicc")$coef-auto.arima(Y$residuals, ic="aic")$coef
# 
# 
# auto.arima(Y$residuals, ic="aicc")$coef
# auto.arima(Y$residuals, ic="aic")$coef
# 
# abs(tmp$coef)<qnorm(1-a/2)*diag(tmp$var.coef)
# #/sqrt(length(X))
# 
# 
# qnorm(1-a/2)*diag(tmp$var.coef)
# tmp$coef


```


```{r IC_vs_lambda}
lamb=c(0, 0.5, 0.75, 1, 2.5)
IC=c("aic","bic","aicc")
mat=matrix(ncol=2*length(IC), nrow=length(lamb))
important=mat
important2=mat
diff.vs.lambda=mat # by sprawdzić, czy rzeczywiście model nie proponuje dodatkowych różnicowań
for(i in 1:3){
  for(j in 1:length(lamb)){
    tmp<-auto.arima(diff(BoxCox(energia, lambda=lamb[j]),lag=12), ic=IC[i])
    mat[j,i]<-tmp$arma[1:4] %>% toString
    important[j,i]<-any(abs(tmp$coef)<qnorm(1-a/2)*diag(tmp$var.coef))
    important2[j,i]<-any(abs(tmp$coef)<qnorm(1-a/2)*diag(tmp$var.coef)/sqrt(length(energia)))
    diff.vs.lambda[j,i]<-all(tmp$arma[6:7]==0)
  }
}
for(i in 4:6){
  for(j in 1:length(lamb)){
    tmp<-auto.arima(tslm(energia ~ poly(trend,1,raw=TRUE) + season, lambda=lamb[j])$residuals,ic=IC[i-3])
    mat[j,i]<-tmp$arma[1:4] %>% toString
    important[j,i]<-any(abs(tmp$coef)<qnorm(1-a/2)*diag(tmp$var.coef))
    important2[j,i]<-any(abs(tmp$coef)<qnorm(1-a/2)*diag(tmp$var.coef)/sqrt(length(energia)))
    diff.vs.lambda[j,i]<-all(tmp$arma[6:7]==0)
  }
}
# True==Good
#!any(important)
#!any(important2)
#all(diff.vs.lambda)
colnames(mat)<-rep(IC,2)   
row.names(mat)<-lamb

kable(mat, caption = "Rzędy modelu SARMA(p,q)(P,Q)[12] dla różnych kryteriów uzyskanych dzięki funkcji auto.arima. Rzędy podane w kolejności: p,q,P,Q") %>%
  kable_styling( full_width = T, font_size = 12,latex_options = "HOLD_position") %>%
  add_header_above(c("lambda"=1,"Różnicowanie" = 3, "Metoda parametryczna" = 3))
```


```{r IC_vs_lambda_nonseasonal}

lamb=c(0, 0.5, 0.75, 1, 2.5)
IC=c("aic","bic","aicc")
mat=matrix(ncol=2*length(IC), nrow=length(lamb))
diff.vs.lambda=mat
for(i in 1:3){
  for(j in 1:length(lamb)){
    tmp<-auto.arima(diff(BoxCox(energia, lambda=lamb[j]),lag=12), ic=IC[i],seasonal = F)
    mat[j,i]<-tmp$arma[1:2] %>% toString
    diff.vs.lambda[j,i]<-all(tmp$arma[6:7]==0)
  }
}
for(i in 4:6){
  for(j in 1:length(lamb)){
    tmp<-auto.arima(tslm(energia ~ poly(trend,1,raw=TRUE) + season, lambda=lamb[j])$residuals,ic=IC[i-3],seasonal = F)
    mat[j,i]<-tmp$arma[1:2] %>% toString
    diff.vs.lambda[j,i]<-all(tmp$arma[6:7]==0)
  }
}

colnames(mat)<-rep(IC,2)   
row.names(mat)<-lamb

kable(mat, caption = "Rzędy modelu ARMA(p,q) dla różnych kryteriów uzyskanych dzięki funkcji auto.arima. Rzędy podane w kolejności: p,q") %>%
  kable_styling( full_width = T, font_size = 12,latex_options = "HOLD_position") %>%
  add_header_above(c("lambda"=1,"Różnicowanie" = 3, "Metoda parametryczna" = 3))


```

Możemy zauważyć, że dla transformacji Box'a-Cox'a z parametrem $\lambda=2.5$, funkcja \verb|auto.arima| zwraca model, dla którego wszystkie rzędy są równe 0 (biały szum), jednak nie jest to poprawne oszacowanie. Rozpatrywane dane po tym przekształceniu sięgają wartości większych niż $2^{33}$, zatem sądzimy, że może to wynikać właśnie z zbyt dużych wartości.

Co ciekawe, poza tym jednym przypadkiem, funkcja \verb|auto.arima| zwraca podobne oszacowania rzędu, niezależnie od zastosowanych przekształceń (w większości rzędy różnią się od siebie łącznie o 1). Jedynie dane po transformacie logarytmicznej czasem dają modele znacznie mniejszych rzędów w metodzie parametrycznej.

Dodatkowo interesujące jest, że jeśli rozpatrzymy jedynie modele ARMA zamiast SARMA, to otrzymane rzędy są znacznie niższe oraz częściej przyjmowane są wyższe rzędy dla ruchomej średniej niż dla autoregresji, gdzie ta relacja była częściej odwrotna przy modelach SARMA. W dodatku zwrócone modele nie zależą od wybranej transformacji Box'a-Cox'a (oprócz $\lambda=2.5$, gdzie ponownie zwracene są modele rzędu 0) oraz jedynie kryterium Bayesowskie dla różnicowania daje inne modele. Reszta kryteriów zwraca ten sam model, nie zależnie czy korzystaliśmy z różnicowania, czy z metody parametrycznej. 

\subsection{Porównanie dopasowania modelu.}
W tej części porównamy metody estymacji wstępnej jak i właściwej. Dla metody różnicowania będziemy porównywać metody estymacji dla modelu $AR(12)$, ponieważ funkcje takie jak \verb|burg| służące do estymacji wstępnej przyjumują tylko jeden parametr, a chcielibyśmy rozważać model $ARMA(2,0)(1,0)[12]$. W przypadku metody parametrycznej wybraliśmy model $ARMA(2,0)=AR(2)$ (wybór dla kryterium bic oraz przekształcenia $\lambda=0.75$).

```{r, results='hide'}
tmp=arima(X, order=c(12,0,0))
abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
tmp=burg(X,12)
abs(tmp$phi)/tmp$se.phi/1.96<1
# w obu przypadkach jedyne istotne współczynniki to 1,2,12
arima(X, order=c(12,0,0), include.mean = F)$coef-burg(X,12)$phi


tmp=arima(Y$residuals, order=c(4,0,12))#, include.mean = F)
abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96
tmp=hannan(Y$residuals,4,12)
abs(tmp$phi)/tmp$se.phi/1.96
abs(tmp$theta)/tmp$se.theta/1.96
# w pierwszej istotny jest TYLKO ma12 jeśli uwzględniamy średnią, lub nic
# w drugiej tylko ma10, ma11, ma12



tmp=arima(Y$residuals, order=c(1,0,2), include.mean = F)
abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
tmp=hannan(Y$residuals,1,2)
abs(tmp$phi)/tmp$se.phi/1.96<1
abs(tmp$theta)/tmp$se.theta/1.96<1
# w tym przypadku, to dla pierwszyej metody wszystko
# dla drugiej ar1 i ma2 (bez ma1)

tmp=hannan(X,1,2)
arima(Y$residuals, order=c(1,0,2), include.mean = F)$coef[1]-tmp$phi
arima(Y$residuals, order=c(1,0,2), include.mean = F)$coef[2:3]-tmp$theta
arima(Y$residuals, order=c(1,0,2), include.mean = F)$coef
c(tmp$phi,tmp$theta)



tmp=arima(Y$residuals, order=c(2,0,0))
abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
tmp=burg(Y$residuals,2)
abs(tmp$phi)/tmp$se.phi/1.96<1
# w obu przypadkach jedyne istotne współczynniki to 1,2,12
arima(Y$residuals, order=c(2,0,0), include.mean = F)$coef-burg(Y$residuals,2)$phi





```
Do estymacji parametrów przy użyciu metody właściwej użyliśmy funkcji \verb|arima|, która wykorzystała w tym celu metodę najmniejszych kwadratów, po czym ponawia estymacje, tym razem metodą największej wiarygodności, gdzie argumentami startowymi w algorytmie są te wyestymowane poprzednią metodą. W przypadku estymacji wstępnej wykorzystaliśmy funkcje \verb|burg| (dla różnicowania) oraz \verb|hannan| (dla metody parametrycznej).

\subsubsection{Różnicowanie}
W przypadku różnicowania obie funkcje zwróciły parametry, gdzie istotne statystycznie (na podstawie kryterium z wykładu, opierającego się na ilorazie estymowanego parametru w stosunku jego błędu) były: ar1, ar2 oraz ar12. Zatem otrzymujemy w tym sposobem model $SARMA(2,0)(1,0)[12]$, który od początku chcieliśmy otrzymać. Dlatego parametry statystycznie nieistotne zamieniliśmy na 0. Zacznijmy od rozważenie modelu opartego na parametrach estymowanych przez funkcję \verb|burg|. Reszty wtedy zachowują się w następujący sposób. 

```{r, results='hide'}

tmp=burg(X,12)
param=tmp$phi
param[abs(tmp$phi)/tmp$se.phi/1.96<1]=0
tmp <- arima(X, order=c(12,0,0),fixed=param,include.mean=F)
```
```{r, fig.cap="Porównanie rezkładu residuów z rozkładem normalnym na wykresie kwantylowym."}
qqnorm(tmp$residuals)
```
Widzimy, że wykres kwantylowy dobrze przybliża linię prostą. W dodatku test Shapiro-Wilka zwrócił p-value na poziomie `r shapiro.test(tmp$residuals)$p.val`, zatem na poziomie istotności $\alpha=0.05$ nie możemy odrzucić hipotezy o normalności reszt. Zastanówmy się jeszcze jak wygląda sytuacja z niezależnością reszt.

```{r, fig.cap="Wykres funkcji autokorelacji dla rozpatrywanych reszt.", fig.height=7}
ggtsdiag(tmp)
```
Na wykresie jedynie jedna wartość wystaje delikatnie poza przedział ufności, zatem reszty przechodzą test białoszumowości. Dodatkowo p-wartości dla tesu Ljungi-Box'a są powyżej wartości krytycznej, zatem nie możemy odrzucić hipotezy o białoszumowości.

Sprawdźmy, czy te testy przejdzie model, którego parametry są estymowane metodą największej wiarygodności.
```{r, results='hide'}
tmp=arima(X, order=c(12,0,0), include.mean = F)
abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
param=tmp$coef
param[abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1]=0
tmp <- arima(X, order=c(12,0,0),fixed=param,include.mean=F)
```
```{r, fig.cap="Porównanie rezkładu residuów z rozkładem normalnym na wykresie kwantylowym."}
qqnorm(tmp$residuals)
```
Widzimy, że wykres kwantylowy dobrze przybliża linię prostą. W dodatku test Shapiro-Wilka zwrócił p-value na poziomie `r shapiro.test(tmp$residuals)$p.val`, zatem na poziomie istotności $\alpha=0.05$ nie możemy odrzucić hipotezy o normalności reszt. Zastanówmy się jeszcze jak wygląda sytuacja z niezależnością reszt.

```{r, fig.cap="Wykres funkcji autokorelacji dla rozpatrywanych reszt.", fig.height=7}
ggtsdiag(tmp)
```
Na wykresie jedynie jedna wartość wystaje delikatnie poza przedział ufności, zatem reszty przechodzą test białoszumowości. Dodatkowo p-wartości dla testu Ljungi-Box'a są powyżej wartości krytycznej, zatem nie możemy odrzucić hipotezy o białoszumowości.

Obie metody przeszły wszystkie testy dając bardzo zbliżone wyniki. Nie jest to dziwne, jeśli spojrzymy, że różnica między parametrami modelu wynosiła kolejno `r (arima(X, order=c(12,0,0), include.mean = F)$coef-burg(X,12)$phi)[c(1,2,12)]`, czyli była o przynajmniej rząd wielkości mniejsza niż wartość parametrów.


\subsubsection{Metoda parametryczna}
Dla tej metody wybraliśmy model $AR(2)$ korzystając z kryterium bic. Wybór padł właśnie na to kryterium, ponieważ przy wyborze innych rozpatrywanych modeli, wiele ze zwracanych parametrów były statystycznie nieistotne, a w przypadku modelu $ARMA(4,12)$, bez uwzględnienia średniej, wszystkie parametry stały się statystycznie nieistotne. Dla metody opartej o funkcję \verb|burg| otrzymaliśmy następujące wyniki.  
```{r, results='hide'}
tmp=burg(Y$residuals,2)
param=tmp$phi
param[abs(tmp$phi)/tmp$se.phi/1.96<1]=0
param
tmp <- arima(Y$residuals, order=c(2,0,0),fixed=param,include.mean=F)
```
```{r, fig.cap="Porównanie rezkładu residuów z rozkładem normalnym na wykresie kwantylowym."}
qqnorm(tmp$residuals)
```
Widzimy, że wykres kwantylowy nie dobrze przybliża linii prostej. W dodatku test Shapiro-Wilka zwrócił p-value na poziomie `r shapiro.test(tmp$residuals)$p.val`, zatem na poziomie istotności $\alpha=0.05$ mamy podstawy do odrzucenia hipotezy o normalności reszt. Zastanówmy się jeszcze jak wygląda sytuacja z niezależnością reszt.

```{r, fig.cap="Wykres funkcji autokorelacji dla rozpatrywanych reszt.", fig.height=7}
ggtsdiag(tmp)
```
Na wykresie jedynie jedna wartość wystaje delikatnie poza przedział ufności, zatem reszty przechodzą test białoszumowości. Dodatkowo p-wartości dla tesu Ljungi-Box'a są powyżej wartości krytycznej, zatem nie możemy odrzucić hipotezy o białoszumowości.

Sprawdźmy czy dostaniemy inne wyniki korzystając z estymacji parametrów metodą największej wiarygodności.
```{r, results='hide'}
tmp=arima(Y$residuals, order=c(2,0,0), include.mean = F)
param=tmp$coef
param
tmp <- arima(Y$residuals, order=c(2,0,0),fixed=param,include.mean=F)
```
```{r, fig.cap="Porównanie rezkładu residuów z rozkładem normalnym na wykresie kwantylowym."}
qqnorm(tmp$residuals)
```
Widzimy, że wykres kwantylowy nie dobrze przybliża linii prostej. W dodatku test Shapiro-Wilka zwrócił p-value na poziomie `r shapiro.test(tmp$residuals)$p.val`, zatem na poziomie istotności $\alpha=0.05$ mamy podstawy do odrzucenia hipotezy o normalności reszt. Zastanówmy się jeszcze jak wygląda sytuacja z niezależnością reszt.

```{r, fig.cap="Wykres funkcji autokorelacji dla rozpatrywanych reszt.", fig.height=7}
ggtsdiag(tmp)
```
Na wykresie jedynie dwie wartość wystaje delikatnie poza przedział ufności (czyli więcej niż przy estymacji wstępnej), zatem reszty przechodzą test białoszumowości. Dodatkowo p-wartości dla tesu Ljungi-Box'a są powyżej wartości krytycznej, zatem nie możemy odrzucić hipotezy o białoszumowości.

W tym przypadku wyniki nadal były podobne, jednak pojawiły się różnice. Różnice między estymowanymi parametrami różniły się o 2 rzędy wielkości.
<!--`r burg(Y$residuals,2)$phi-arima(Y$residuals, order=c(2,0,0), include.mean = F)$coef`-->
Jeśli postanowilibyśmy wybrać model z tabeli 9 -- $ARMA(1,2)$ -- otrzymalibyśmy wyniki bardzo zbliżone do wyników otrzymanych powyżej, jednak p-wartości były by wyższe (ale nadal znacznie <!--dwukrotnie--> niższe niż poziom krytyczny).

\subsection{Porównanie z funkcjami acf i pacf}
Możemy rozpatrzyć jeszcze jak wyglądają funkcje acf i pacf dla wybranych modeli.
```{r}
tmp=arima(X, order=c(12,0,0), include.mean = F)
#abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
param=tmp$coef
param[abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1]=0
#param
N=20000
acf.sim.X<-1:N %>%
  sapply( function(g) Acf(arima.sim(n = length(X), list(ar = param)), plot=F, lag.max = 24)$acf[2:25]) %>%
  apply(1,mean)
pacf.sim.X<-1:N %>%
  sapply( function(g) pacf(arima.sim(n = length(X), list(ar = param)), plot=F, lag.max = 24)$acf) %>%
  apply(1,mean) 


tmp=arima(Y$residuals, order=c(2,0,0), include.mean = F)
#abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
param=tmp$coef
param[abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1]=0
#param

acf.sim.Y<-1:N %>%
  sapply( function(g) Acf(arima.sim(n = length(Y$residuals), list(ar = param)), plot=F, lag.max = 24)$acf[2:25]) %>%
  apply(1,mean)
pacf.sim.Y<-1:N %>%
  sapply( function(g) pacf(arima.sim(n = length(Y$residuals), list(ar = param)), plot=F, lag.max = 24)$acf) %>%
  apply(1,mean)  


tmp=arima(Y$residuals, order=c(5,0,0), seasonal=c(0,0,1), include.mean = F)
#abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
param=tmp$coef
#param
#
acf.sim.Y.s<-1:N %>%
  sapply( function(g) Acf(arima.sim(n = length(Y$residuals), list(ar = param[1:5], sma=param[6])), plot=F, lag.max = 24)$acf[2:25]) %>%
  apply(1,mean)
pacf.sim.Y.s<-1:N %>%
  sapply( function(g) pacf(arima.sim(n = length(Y$residuals), list(ar = param[1:5], sma=param[6])), plot=F, lag.max = 24)$acf) %>%
  apply(1,mean)  


```
\subsubsection{Różnicowanie}
Zacznijmy od porównania funkcji acf i pacf danych po zróżnicowaniu (i oczywiście innych używanych transformatach) z wysymulowanymi wartościami teoretycznymi dla rozpatrywanego modelu $SARMA(2,0)(1,0)[12]$. Wykresy prezentują się następująco.
```{r, fig.cap="Wykresy pacf dla rozpatrywanych danych (czarne słupki) porównane z danymi wysymulawanymi (niebieskie kropki)."}
ggPacf(X,lag.max = 24, main="") + 
  geom_point(
    mapping = aes(x = 1:24, y = pacf.sim.X),color="blue"
    )
```
```{r, fig.cap="Wykresy acf dla rozpatrywanych danych (czarne słupki) porównane z danymi wysymulawanymi (niebieskie kropki)."}
ggAcf(X,lag.max = 24, main="") + 
  geom_point(
    mapping = aes(x = 1:24, y = acf.sim.X),color="blue"
    )
```
Możemy zauważyć, że empiryczna funkcja pacf dobrze przybliża dane uzyskane podczas symulacji. Niestety funkcje acf pokrywają się jedynie dla pierwszych paru (około 8) opóźnień. 

\subsubsection{Model parametryczny}
Porównajmy teraz funkcje acf i pacf, ale dla danych po dekompozycji metodą parametryczną (i oczywiście innych używanych transformatach) z wysymulowanymi wartościami teoretycznymi dla rozpatrywanego modelu $ARMA(2,0)$. Wykresy prezentują się następująco.
```{r, fig.cap="Wykresy pacf dla rozpatrywanych danych (czarne słupki) porównane z danymi wysymulawanymi (niebieskie kropki)."}
ggPacf(Y$residuals,lag.max = 24, main="") + 
  geom_point(
    mapping = aes(x = 1:24, y = pacf.sim.Y),color="blue"
    )
```
```{r, fig.cap="Wykresy acf dla rozpatrywanych danych (czarne słupki) porównane z danymi wysymulawanymi (niebieskie kropki)."}
ggAcf(Y$residuals,lag.max = 24, main="") + 
  geom_point(
    mapping = aes(x = 1:24, y = acf.sim.Y),color="blue"
    )
```
Możemy zauważyć, że empiryczna funkcja pacf dobrze przybliża dane uzyskane podczas symulacji. Dla większości opóźnień funkcja mieści się w przedziałach istotności oraz dla tych opóźnień symulacje zwróciły wartości bliskie 0. Podobna sytuacja jest dla funkcji acf. Jednak obie funkcje mają problem z opóźnieniem $h=3$, gdzie rozpatrywane wykresy zwracają znacząco różniące się wartości.

\subsubsection{Model parametryczny z sezonowością}
Poprzedni punkt był dla modelu uzyskanego przy pomocy kryterium Bayesowskiego. Sprawdźmy jak zmienią się wykresy, jeżeli skorzystamy z kreyterium Akkaike. W tym przypadku rozpatrujemy model $SARMA(5,0)(0,1)[12]$ i wykresy wyglądają następująco.
```{r, fig.cap="Wykresy pacf dla rozpatrywanych danych (czarne słupki) porównane z danymi wysymulawanymi (niebieskie kropki)."}
ggPacf(Y$residuals,lag.max = 24, main="") + 
  geom_point(
    mapping = aes(x = 1:24, y = pacf.sim.Y.s),color="blue"
    )
```
```{r, fig.cap="Wykresy acf dla rozpatrywanych danych (czarne słupki) porównane z danymi wysymulawanymi (niebieskie kropki)."}
ggAcf(Y$residuals,lag.max = 24, main="") + 
  geom_point(
    mapping = aes(x = 1:24, y = acf.sim.Y.s),color="blue"
    )
```
W tym przypadku straciliśmy (nieznacznie) dopasowanie funkcji acf, ale za to otrzymaliśmy znaczne poprawienie dopasowania funkcji pacf dla początkowych wartości. Jednak nadal, jak w poprzednim przypadku, dla opóźnienia $h=14$ i funkcji pacf symulowane wartości znacząco się różnią od empirycznych.

\subsection{Predykcja}
Mając już odpowiednie modele możemy przystąpić do predykcji zużycia energii w przyszłych latach. Dla różnicowania rozpatrujemy model $SARMA(2,0)(1,0)[12]$, natomiast dla metody parametrycznej model $ARMA(2,0)$. W wyniku dostaliśmy poniższe prognozy.
```{r, fig.cap="Prognoza dla różnicowania"}
tmp=arima(X, order=c(12,0,0), include.mean = F)
#abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96<1
param=tmp$coef
param<-param[abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96>1]
#param
predict(arima(X, order=c(2,0,0), seasonal = c(1,0,0), fixed = param, include.mean = F), n.ahead = 24)$pred %>%
  diffinv(lag=12,xi=BoxCox(energia,lambda=0)[(length(energia)-11):(length(energia))]) %>%
  BoxCoxInv(lambda=0)-> prognoza
autoplot(cbind(prognoza,energia)) + ylab("Energia")
```
```{r, fig.cap="Prognoza dla metody parametrycznej"}
tmp=arima(Y$residuals, order=c(2,0,0), include.mean = F)
param=tmp$coef
param<-param[abs(tmp$coef)/sqrt(diag(tmp$var.coef))/1.96>1]
#param
tmp=predict(arima(Y$residuals, order=c(2,0,0), fixed = param, include.mean = F), n.ahead = 24)$pred

tmp<-tmp+Y$coef[1]+Y$coef[2]*((length(Y)+1):(length(Y)+24))+c(Y$coef[10:13],0,Y$coef[3:9]) # sezonowość dodana dziwnie, by uwzględnić, że przwidujemy od września

prognoza<-BoxCoxInv(tmp, lambda=0.75)

autoplot(cbind(prognoza,energia)) + ylab("Energia")
```
Możemy zobaczyć, że w obu przypadkach otrzymaliśmy podobne wyniki. Dla różnicowania dostaliśmy większe wyniki oraz ta metoda odwzorowała lepiej zachowanie zużycia przed momentami największego zużycia energii, porównując z ostatnimi latami(w ostatnich latach zużycie w tym okresie nie różniło się tak bardzo jak w poprzednich, co udało się odwzorować).

\subsection{Podsumowanie}
Najlepiej dopasowane były modele uwzględniające sezonowość, czyli modele SARMA. Dla tych modeli symulacje funkcji acf i pacf dobrze pokrywały dane empiryczne. Porównując metody dekompozycji, lepsza okazała się ta oparta na różnicowaniu sezonowym. Reszty otrzymanego modelu przechodziły wszystkie testy (graficzne i formalne) na białoszumowość i normalność, czego nie otrzymaliśmy korzystając z metod parametrycznych. W dodatku dopasowanie modelu okazało się przyjemniejszy. Przy metodzie parametrycznej otrzymywaliśmy najczęściej modele ARMA, natomiast przy różnicowaniu pojawił się model AR, który jest dużo prostszy w modelowaniu i interpretacji (przykładowo funkcji acf i pacf, w modelu mieszanym dochodzą dodatkowe zależności). Jednak niezależnie od rozpatrywanych metod dekompozycji, jak i estymacji, zawsze otrzymywaliśmy duże wartości funckji pacf dla opóźnienia $h=14$.







